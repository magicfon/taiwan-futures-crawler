#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å°ç£æœŸè²¨äº¤æ˜“æ‰€è³‡æ–™çˆ¬å–å·¥å…· (Python ç‰ˆæœ¬)
å¯ç”¨æ–¼å¿«é€Ÿçˆ¬å–æ•´å¹´çš„æ­·å²è³‡æ–™
"""

import requests
import pandas as pd
from bs4 import BeautifulSoup
import os
import time
import argparse
import datetime
import re
import concurrent.futures
from tqdm import tqdm
import logging
import random
import json
from pathlib import Path
try:
    from database_manager import TaifexDatabaseManager
    from daily_report_generator import DailyReportGenerator
    from google_sheets_manager import GoogleSheetsManager
    from telegram_notifier import TelegramNotifier
    from chart_generator import ChartGenerator
    DB_AVAILABLE = True
    SHEETS_AVAILABLE = True
    TELEGRAM_AVAILABLE = True
    CHART_AVAILABLE = True
except ImportError:
    DB_AVAILABLE = False
    SHEETS_AVAILABLE = False
    TELEGRAM_AVAILABLE = False
    CHART_AVAILABLE = False
    print("è­¦å‘Š: è³‡æ–™åº«æˆ–Google Sheetsæ¨¡çµ„æœªæ‰¾åˆ°ï¼Œå°‡ä½¿ç”¨å‚³çµ±æª”æ¡ˆå­˜å„²æ–¹å¼")

# è¨­å®šåŸºæœ¬åƒæ•¸
BASE_URL = 'https://www.taifex.com.tw/cht/3/futContractsDate'
# å¯çˆ¬å–çš„æœŸè²¨å¥‘ç´„ä»£ç¢¼: å°æŒ‡æœŸ, é›»å­æœŸ, å°å‹å°æŒ‡æœŸ, å¾®å‹å°æŒ‡æœŸ, ç¾åœ‹é‚£æ–¯é”å…‹100æœŸè²¨
CONTRACTS = ['TX', 'TE', 'MTX', 'ZMX', 'NQF']
# å¥‘ç´„åç¨±å°ç…§è¡¨
CONTRACT_NAMES = {
    'TX': 'è‡ºè‚¡æœŸè²¨',
    'TE': 'é›»å­æœŸè²¨',
    'MTX': 'å°å‹è‡ºæŒ‡æœŸè²¨',
    'ZMX': 'å¾®å‹è‡ºæŒ‡æœŸè²¨',
    'NQF': 'ç¾åœ‹é‚£æ–¯é”å…‹100æœŸè²¨'
}
# èº«ä»½åˆ¥åˆ—è¡¨
IDENTITIES = ['è‡ªç‡Ÿå•†', 'æŠ•ä¿¡', 'å¤–è³‡']

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("taifex_crawler.log", encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("æœŸè²¨çˆ¬èŸ²")

class TaifexCrawler:
    def __init__(self, output_dir="output", max_retries=3, delay=0.5, 
                 max_workers=10, timeout=30, use_proxy=False):
        """
        åˆå§‹åŒ–çˆ¬èŸ²
        
        Args:
            output_dir: è¼¸å‡ºç›®éŒ„
            max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
            delay: è«‹æ±‚é–“éš”æ™‚é–“ (ç§’)
            max_workers: æœ€å¤§å·¥ä½œç·šç¨‹æ•¸
            timeout: è«‹æ±‚è¶…æ™‚æ™‚é–“ (ç§’)
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†
        """
        self.output_dir = output_dir
        self.max_retries = max_retries
        self.delay = delay
        self.max_workers = max_workers
        self.timeout = timeout
        self.use_proxy = use_proxy
        self.session = requests.Session()
        
        # è¨­å®šè«‹æ±‚æ¨™é ­
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive',
            'Referer': 'https://www.taifex.com.tw/cht/3/futDailyMarketView',
        }
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
    
    def fetch_data(self, date_str, contract, query_type='2', market_code='0', identity=None):
        """
        å¾æœŸäº¤æ‰€ç²å–ç‰¹å®šæ—¥æœŸå’Œå¥‘ç´„çš„è³‡æ–™
        
        Args:
            date_str: æ—¥æœŸå­—ä¸²ï¼Œæ ¼å¼ç‚º 'yyyy/MM/dd'
            contract: å¥‘ç´„ä»£ç¢¼ (TX, TE, MTX, ZMX, NQF)
            query_type: æŸ¥è©¢é¡å‹ ('1', '2', '3')
            market_code: å¸‚å ´ä»£ç¢¼ ('0', '1', '2')
            identity: èº«ä»½åˆ¥ (è‡ªç‡Ÿå•†, æŠ•ä¿¡, å¤–è³‡)
            
        Returns:
            è§£æå¾Œçš„è³‡æ–™å­—å…¸æˆ– None (å¦‚æœç„¡è³‡æ–™)
        """
        # æª¢æŸ¥æ—¥æœŸæ˜¯å¦è¶…éä»Šå¤©
        try:
            target_date = datetime.datetime.strptime(date_str, "%Y/%m/%d")
            today = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
            
            if target_date > today:
                logger.warning(f"æ—¥æœŸ {date_str} è¶…éä»Šå¤©ï¼Œè·³éçˆ¬å–")
                return None
        except Exception as e:
            logger.error(f"æ—¥æœŸæ ¼å¼æª¢æŸ¥éŒ¯èª¤: {str(e)}")
            # å¿½ç•¥éŒ¯èª¤ï¼Œç¹¼çºŒåŸ·è¡Œ
        
        for retry in range(self.max_retries):
            try:
                # æ§‹å»ºæŸ¥è©¢åƒæ•¸
                params = {
                    'queryType': query_type,
                    'marketCode': market_code,
                    'dateaddcnt': '',
                    'commodity_id': contract,
                    'queryDate': date_str
                }
                
                # æ·»åŠ éš¨æ©Ÿå»¶é²ï¼Œé¿å…è¢«å°IP
                time.sleep(self.delay + random.uniform(0, 0.5))
                
                # ç™¼é€ GET è«‹æ±‚
                response = self.session.get(
                    BASE_URL, 
                    params=params, 
                    headers=self.headers,
                    timeout=self.timeout
                )
                
                # æª¢æŸ¥éŸ¿æ‡‰ç‹€æ…‹
                if response.status_code != 200:
                    logger.warning(f"è«‹æ±‚å¤±æ•—: {response.status_code}, é‡è©¦ä¸­ ({retry+1}/{self.max_retries})")
                    continue
                
                # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤è¨Šæ¯æˆ–ç„¡è³‡æ–™è¨Šæ¯
                html_content = response.text
                if self._has_no_data_message(html_content):
                    logger.info(f"{date_str} {contract} ç„¡äº¤æ˜“è³‡æ–™")
                    return None
                
                if self._has_error_message(html_content):
                    logger.warning(f"{date_str} {contract} é é¢åŒ…å«éŒ¯èª¤è¨Šæ¯")
                    continue
                
                # è§£æè³‡æ–™
                if identity:
                    data = self._parse_identity_data(html_content, contract, date_str, identity)
                    if not data:
                        logger.warning(f"{date_str} {contract} æ‰¾ä¸åˆ°èº«ä»½åˆ¥ {identity} çš„è³‡æ–™")
                else:
                    data = self._parse_contract_data(html_content, contract, date_str)
                    if not data:
                        logger.warning(f"{date_str} {contract} æ‰¾ä¸åˆ°å¥‘ç´„è³‡æ–™")
                
                if data:
                    return data
                
                logger.warning(f"{date_str} {contract} è³‡æ–™è§£æå¤±æ•—ï¼Œé‡è©¦ä¸­ ({retry+1}/{self.max_retries})")
            
            except Exception as e:
                logger.error(f"çˆ¬å– {date_str} {contract} è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                if retry < self.max_retries - 1:
                    logger.info(f"ç­‰å¾…å¾Œé‡è©¦ ({retry+1}/{self.max_retries})")
                    time.sleep(self.delay * 2)
        
        return None
    
    def _has_no_data_message(self, html_content):
        """æª¢æŸ¥ç¶²é æ˜¯å¦é¡¯ç¤ºã€Œç„¡äº¤æ˜“è³‡æ–™ã€è¨Šæ¯"""
        no_data_patterns = [
            'æŸ¥ç„¡è³‡æ–™', 'ç„¡äº¤æ˜“è³‡æ–™', 'å°šç„¡è³‡æ–™', 'ç„¡æ­¤è³‡æ–™',
            'No data', 'ä¸å­˜åœ¨', 'ä¸æä¾›', 'ç¶­è­·ä¸­'
        ]
        return any(pattern in html_content for pattern in no_data_patterns)
    
    def _has_error_message(self, html_content):
        """æª¢æŸ¥ç¶²é æ˜¯å¦é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯"""
        error_patterns = [
            'ç³»çµ±ç™¼ç”ŸéŒ¯èª¤', 'æŸ¥è©¢ç™¼ç”ŸéŒ¯èª¤', 'æ—¥æœŸéŒ¯èª¤', 
            'è«‹è¼¸å…¥æ­£ç¢ºçš„æ—¥æœŸ', 'Error occurred', 'è³‡æ–™éŒ¯èª¤',
            'éŒ¯èª¤ä»£ç¢¼', 'ä¼ºæœå™¨éŒ¯èª¤', 'Server Error', 'ç„¡æ³•è™•ç†æ‚¨çš„è«‹æ±‚'
        ]
        return any(pattern in html_content for pattern in error_patterns)
    
    def _parse_number(self, text):
        """è§£æè¡¨æ ¼ä¸­çš„æ•¸å­—ï¼Œè™•ç†åƒä½åˆ†éš”ç¬¦å’Œå°æ•¸é»"""
        if not text:
            return 0
        
        try:
            # ç§»é™¤æ‰€æœ‰éæ•¸å­—å­—ç¬¦ï¼ˆä¿ç•™æ¸›è™Ÿè¡¨ç¤ºè² æ•¸å’Œå°æ•¸é»ï¼‰
            cleaned_str = re.sub(r'[^\d\-\.]', '', str(text))
            
            # æª¢æŸ¥æ˜¯å¦æœ‰å°æ•¸é»
            if '.' in cleaned_str:
                # è§£æç‚ºæµ®é»æ•¸
                return float(cleaned_str) if cleaned_str else 0
            else:
                # è½‰æ›ç‚ºæ•´æ•¸
                return int(cleaned_str) if cleaned_str else 0
        except:
            # å¦‚æœè§£æå¤±æ•—ï¼Œå˜—è©¦é¡å¤–çš„è™•ç†ï¼ˆä¾‹å¦‚ç§‘å­¸è¨˜æ•¸æ³•ï¼‰
            try:
                return float(text.replace(',', ''))
            except:
                return 0
    
    def _parse_contract_data(self, html_content, contract, date_str):
        """è§£æåŸºæœ¬å¥‘ç´„è³‡æ–™"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # å°‹æ‰¾ä¸»è¦è³‡æ–™è¡¨æ ¼
            tables = soup.find_all('table', {'class': 'table_f'})
            if not tables:
                tables = soup.find_all('table')
            
            if not tables:
                return None
            
            # é¸æ“‡æœ€å¤§çš„è¡¨æ ¼
            main_table = max(tables, key=lambda t: len(str(t)))
            rows = main_table.find_all('tr')
            
            results = []
            current_contract = None
            
            # å¥‘ç´„åç¨±å°æ‡‰
            contract_mapping = {
                'TX': 'è‡ºè‚¡æœŸè²¨',
                'TE': 'é›»å­æœŸè²¨', 
                'MTX': 'å°å‹è‡ºæŒ‡',
                'ZMX': 'å¾®å‹è‡ºæŒ‡',
                'NQF': 'é‚£æ–¯é”å…‹'
            }
            
            target_contract_name = contract_mapping.get(contract, contract)
            
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                if len(cells) < 3:
                    continue
                    
                # å–å¾—æ‰€æœ‰å–®å…ƒæ ¼æ–‡å­—
                cell_texts = [cell.get_text(strip=True) for cell in cells]
                
                # æª¢æŸ¥æ˜¯å¦æ˜¯ç›®æ¨™å¥‘ç´„çš„èµ·å§‹è¡Œ
                if len(cell_texts) >= 3 and target_contract_name in cell_texts[1]:
                    current_contract = target_contract_name
                    logger.debug(f"æ‰¾åˆ°å¥‘ç´„: {current_contract}")
                    
                    # è§£æé€™è¡Œè³‡æ–™ (ç¬¬ä¸€å€‹èº«ä»½åˆ¥ï¼Œé€šå¸¸æ˜¯è‡ªç‡Ÿå•†)
                    if len(cell_texts) >= 8:
                        identity = cell_texts[2]  # èº«ä»½åˆ¥
                        data = self._build_data_dict(date_str, contract, identity, cell_texts, 3)
                        if data:
                            results.append(data)
                            
                # æª¢æŸ¥æ˜¯å¦æ˜¯åŒå¥‘ç´„çš„å…¶ä»–èº«ä»½åˆ¥è¡Œ (æŠ•ä¿¡ã€å¤–è³‡)
                elif current_contract and len(cell_texts) >= 8:
                    # å¦‚æœç¬¬ä¸€åˆ—ä¸æ˜¯æ•¸å­—ï¼ˆåºè™Ÿï¼‰ï¼Œå¯èƒ½æ˜¯èº«ä»½åˆ¥è³‡æ–™
                    if not cell_texts[0].isdigit() and cell_texts[0] in ['æŠ•ä¿¡', 'å¤–è³‡', 'æŠ•é¡§']:
                        identity = cell_texts[0]  # èº«ä»½åˆ¥
                        data = self._build_data_dict(date_str, contract, identity, cell_texts, 1)
                        if data:
                            results.append(data)
                    
                    # å¦‚æœé‡åˆ°ä¸‹ä¸€å€‹å¥‘ç´„ï¼Œé‡ç½®current_contract
                    elif cell_texts[0].isdigit() and len(cell_texts) > 1:
                        for contract_name in contract_mapping.values():
                            if contract_name in cell_texts[1]:
                                current_contract = None  # é‡ç½®ï¼Œè¡¨ç¤ºå·²ç¶“åˆ°ä¸‹ä¸€å€‹å¥‘ç´„
                                break
            
            # å¦‚æœæ‰¾åˆ°å¤šç­†è³‡æ–™ï¼Œè¿”å›ç¬¬ä¸€ç­†ä½œç‚ºä»£è¡¨ï¼ˆæˆ–å¯ä»¥åˆä½µï¼‰
            if results:
                return results[0]  # è¿”å›è‡ªç‡Ÿå•†çš„è³‡æ–™ä½œç‚ºä»£è¡¨
            
            return None
            
        except Exception as e:
            logger.error(f"è§£æ {date_str} {contract} è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
    
    def _build_data_dict(self, date_str, contract, identity, cell_texts, start_idx):
        """æ§‹å»ºè³‡æ–™å­—å…¸"""
        try:
            data = {
                'æ—¥æœŸ': date_str,
                'å¥‘ç´„åç¨±': contract,
                'èº«ä»½åˆ¥': identity,
                'å¤šæ–¹äº¤æ˜“å£æ•¸': self._parse_number(cell_texts[start_idx]) if len(cell_texts) > start_idx else 0,
                'å¤šæ–¹å¥‘ç´„é‡‘é¡': self._parse_number(cell_texts[start_idx+1]) if len(cell_texts) > start_idx+1 else 0,
                'ç©ºæ–¹äº¤æ˜“å£æ•¸': self._parse_number(cell_texts[start_idx+2]) if len(cell_texts) > start_idx+2 else 0,
                'ç©ºæ–¹å¥‘ç´„é‡‘é¡': self._parse_number(cell_texts[start_idx+3]) if len(cell_texts) > start_idx+3 else 0,
                'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸': self._parse_number(cell_texts[start_idx+4]) if len(cell_texts) > start_idx+4 else 0,
                'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡': self._parse_number(cell_texts[start_idx+5]) if len(cell_texts) > start_idx+5 else 0,
                'å¤šæ–¹æœªå¹³å€‰å£æ•¸': self._parse_number(cell_texts[start_idx+6]) if len(cell_texts) > start_idx+6 else 0,
                'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': self._parse_number(cell_texts[start_idx+7]) if len(cell_texts) > start_idx+7 else 0,
                'ç©ºæ–¹æœªå¹³å€‰å£æ•¸': self._parse_number(cell_texts[start_idx+8]) if len(cell_texts) > start_idx+8 else 0,
                'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': self._parse_number(cell_texts[start_idx+9]) if len(cell_texts) > start_idx+9 else 0,
                'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸': self._parse_number(cell_texts[start_idx+10]) if len(cell_texts) > start_idx+10 else 0,
                'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡': self._parse_number(cell_texts[start_idx+11]) if len(cell_texts) > start_idx+11 else 0
            }
            return data
        except Exception as e:
            logger.error(f"æ§‹å»ºè³‡æ–™å­—å…¸æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None
    
    def _parse_identity_data(self, html_content, contract, date_str, identity):
        """è§£æç‰¹å®šèº«ä»½åˆ¥çš„è³‡æ–™"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # å°‹æ‰¾ä¸»è¦è³‡æ–™è¡¨æ ¼
            tables = soup.find_all('table', {'class': 'table_f'})
            if not tables:
                tables = soup.find_all('table')
            
            if not tables:
                return None
            
            # é¸æ“‡æœ€å¤§çš„è¡¨æ ¼
            main_table = max(tables, key=lambda t: len(str(t)))
            
            # å…ˆæ‰¾åˆ°å°æ‡‰å¥‘ç´„çš„è‡ªç‡Ÿå•†è¡Œ
            base_row = None
            rows = main_table.find_all('tr')
            
            # é‡å°ä¸åŒçš„å¥‘ç´„ä½¿ç”¨ä¸åŒçš„é—œéµè©
            contract_keywords = {
                'TX': ['è‡ºæŒ‡æœŸ', 'å°æŒ‡æœŸ', 'è‡ºè‚¡æœŸ', 'å°è‚¡æœŸ'],
                'TE': ['é›»å­æœŸ'],
                'MTX': ['å°å‹è‡ºæŒ‡', 'å°å‹å°æŒ‡', 'å°è‡ºæŒ‡', 'å°å°æŒ‡'],
                'ZMX': ['å¾®å‹è‡ºæŒ‡', 'å¾®å‹å°æŒ‡', 'å¾®è‡ºæŒ‡', 'å¾®å°æŒ‡'],
                'NQF': ['é‚£æ–¯é”å…‹', 'Nasdaq', 'ç¾åœ‹é‚£æ–¯é”å…‹'],
            }
            
            # æ‰¾åˆ°è‡ªç‡Ÿå•†è¡Œ
            for i, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                if not cells:
                    continue
                
                row_text = ' '.join([cell.get_text(strip=True) for cell in cells])
                
                # ç²¾ç¢ºåŒ¹é…ç‰¹å®šå¥‘ç´„
                if contract in row_text or CONTRACT_NAMES.get(contract, '') in row_text:
                    # ç¢ºèªæ˜¯å¦åŒ…å«è‡ªç‡Ÿå•†èº«ä»½åˆ¥
                    if 'è‡ªç‡Ÿå•†' in row_text and (
                        contract in row_text or 
                        any(keyword in row_text for keyword in contract_keywords.get(contract, []))
                    ):
                        base_row = i
                        break
            
            if base_row is None:
                logger.warning(f"ç„¡æ³•æ‰¾åˆ° {contract} è‡ªç‡Ÿå•†è¡Œï¼Œç„¡æ³•ç¢ºå®šå…¶ä»–èº«ä»½åˆ¥ä½ç½®")
                return None
            
            # æ ¹æ“šèº«ä»½åˆ¥ç¢ºå®šè¦å–çš„è¡Œ
            target_index = None
            if identity == 'è‡ªç‡Ÿå•†':
                target_index = base_row
            elif identity == 'æŠ•ä¿¡':
                target_index = base_row + 1
            elif identity == 'å¤–è³‡':
                target_index = base_row + 2
            else:
                return None
            
            # ç¢ºä¿ç›®æ¨™ç´¢å¼•æœ‰æ•ˆ
            if target_index < 0 or target_index >= len(rows):
                logger.warning(f"{contract} {identity} ç›®æ¨™è¡Œç´¢å¼•è¶…å‡ºç¯„åœ: {target_index}")
                return None
            
            # å–å¾—ç›®æ¨™è¡Œ
            target_row = rows[target_index]
            
            # è§£æè¡Œä¸­çš„è³‡æ–™
            cells = target_row.find_all('td')
            if len(cells) < 5:
                return None
            
            # ç²å–æ‰€æœ‰å–®å…ƒæ ¼æ–‡æœ¬
            cell_texts = [cell.get_text(strip=True) for cell in cells]
            
            # å˜—è©¦ä½¿ç”¨è¡¨é ­æŸ¥æ‰¾æ¬„ä½ä½ç½®
            column_indices = self._find_column_indices(main_table)
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºç¬¬ä¸€æ¬„åŒ…å«èº«ä»½åˆ¥çš„æ ¼å¼
            is_identity_first = cell_texts[0] == identity or identity in cell_texts[0]
            
            # æŒ‡å®šé è¨­æ¬„ä½ä½ç½®
            if is_identity_first:
                # å¦‚æœèº«ä»½åˆ¥åœ¨ç¬¬ä¸€æ¬„
                indices = {
                    'å¤šæ–¹äº¤æ˜“å£æ•¸': 1,
                    'å¤šæ–¹å¥‘ç´„é‡‘é¡': 2,
                    'ç©ºæ–¹äº¤æ˜“å£æ•¸': 3,
                    'ç©ºæ–¹å¥‘ç´„é‡‘é¡': 4,
                    'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸': 5,
                    'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡': 6,
                    'å¤šæ–¹æœªå¹³å€‰å£æ•¸': 7,
                    'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': 8,
                    'ç©ºæ–¹æœªå¹³å€‰å£æ•¸': 9,
                    'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': 10,
                    'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸': 11,
                    'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡': 12
                }
            else:
                # æ¨™æº–æ ¼å¼
                indices = {
                    'å¤šæ–¹äº¤æ˜“å£æ•¸': 3,
                    'å¤šæ–¹å¥‘ç´„é‡‘é¡': 4,
                    'ç©ºæ–¹äº¤æ˜“å£æ•¸': 5,
                    'ç©ºæ–¹å¥‘ç´„é‡‘é¡': 6,
                    'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸': 7,
                    'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡': 8,
                    'å¤šæ–¹æœªå¹³å€‰å£æ•¸': 9,
                    'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': 10,
                    'ç©ºæ–¹æœªå¹³å€‰å£æ•¸': 11,
                    'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': 12,
                    'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸': 13,
                    'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡': 14
                }
                
                # å¦‚æœåˆ—æ•¸ä¸è¶³ï¼Œå˜—è©¦æ›´ç²¾ç°¡çš„æ ¼å¼
                if len(cell_texts) < 14:
                    indices = {
                        'å¤šæ–¹äº¤æ˜“å£æ•¸': 2,
                        'å¤šæ–¹å¥‘ç´„é‡‘é¡': -1,
                        'ç©ºæ–¹äº¤æ˜“å£æ•¸': 3,
                        'ç©ºæ–¹å¥‘ç´„é‡‘é¡': -1,
                        'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸': 4,
                        'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡': -1,
                        'å¤šæ–¹æœªå¹³å€‰å£æ•¸': 5,
                        'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': -1,
                        'ç©ºæ–¹æœªå¹³å€‰å£æ•¸': 6,
                        'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': -1,
                        'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸': 7,
                        'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡': -1
                    }
            
            # å¦‚æœæ‰¾åˆ°è¡¨é ­å®šç¾©çš„æ¬„ä½ï¼Œå‰‡ä½¿ç”¨
            if column_indices:
                indices.update(column_indices)
            
            # å®‰å…¨ç²å–æ•¸å€¼
            def safe_get(field):
                idx = indices.get(field, -1)
                if 0 <= idx < len(cell_texts):
                    return self._parse_number(cell_texts[idx])
                return 0
            
            # æ§‹å»ºè³‡æ–™å­—å…¸
            data = {
                'æ—¥æœŸ': date_str,
                'å¥‘ç´„åç¨±': contract,
                'èº«ä»½åˆ¥': identity,
                'å¤šæ–¹äº¤æ˜“å£æ•¸': safe_get('å¤šæ–¹äº¤æ˜“å£æ•¸'),
                'å¤šæ–¹å¥‘ç´„é‡‘é¡': safe_get('å¤šæ–¹å¥‘ç´„é‡‘é¡'),
                'ç©ºæ–¹äº¤æ˜“å£æ•¸': safe_get('ç©ºæ–¹äº¤æ˜“å£æ•¸'),
                'ç©ºæ–¹å¥‘ç´„é‡‘é¡': safe_get('ç©ºæ–¹å¥‘ç´„é‡‘é¡'),
                'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸': safe_get('å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸'),
                'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡': safe_get('å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡'),
                'å¤šæ–¹æœªå¹³å€‰å£æ•¸': safe_get('å¤šæ–¹æœªå¹³å€‰å£æ•¸'),
                'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': safe_get('å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'),
                'ç©ºæ–¹æœªå¹³å€‰å£æ•¸': safe_get('ç©ºæ–¹æœªå¹³å€‰å£æ•¸'),
                'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡': safe_get('ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'),
                'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸': safe_get('å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸'),
                'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡': safe_get('å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡')
            }
            
            # è¨˜éŒ„æ—¥èªŒä¿¡æ¯
            logger.debug(f"ä½¿ç”¨çµ•å°ä½ç½®è§£æ {contract} {identity}ï¼Œåœ¨ç¬¬ {target_index} è¡Œ")
            
            return data
            
        except Exception as e:
            logger.error(f"è§£æ {date_str} {contract} {identity} è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
            return None

    def crawl_single_day(self, date_str, contracts=None, identities=None):
        """çˆ¬å–å–®æ—¥çš„è³‡æ–™"""
        if not contracts:
            contracts = CONTRACTS
        
        results = []
        
        # çˆ¬å–åŸºæœ¬è³‡æ–™ï¼ˆéèº«ä»½åˆ¥ï¼‰
        if not identities:
            for contract in contracts:
                data = self.fetch_data(date_str, contract)
                if data:
                    results.append(data)
        
        # çˆ¬å–å¤šèº«ä»½åˆ¥è³‡æ–™
        else:
            for contract in contracts:
                for identity in identities:
                    data = self.fetch_data(date_str, contract, identity=identity)
                    if data:
                        results.append(data)
        
        return results

    def _is_business_day(self, date):
        """æª¢æŸ¥æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼ˆéé€±æœ«ï¼‰"""
        day = date.weekday()
        # 0-4 æ˜¯é€±ä¸€åˆ°é€±äº”ï¼Œ5-6 æ˜¯é€±æœ«
        return day < 5

    def crawl_date_range(self, start_date, end_date, contracts=None, identities=None):
        """
        çˆ¬å–æŒ‡å®šæ—¥æœŸç¯„åœçš„è³‡æ–™
        
        Args:
            start_date: é–‹å§‹æ—¥æœŸ (datetime object)
            end_date: çµæŸæ—¥æœŸ (datetime object)
            contracts: è¦çˆ¬å–çš„å¥‘ç´„åˆ—è¡¨ï¼Œé»˜èªç‚ºæ‰€æœ‰å¥‘ç´„
            identities: è¦çˆ¬å–çš„èº«ä»½åˆ¥åˆ—è¡¨ï¼Œé»˜èªç‚ºä¸çˆ¬å–èº«ä»½åˆ¥è³‡æ–™
            
        Returns:
            DataFrame åŒ…å«æ‰€æœ‰çˆ¬å–çš„è³‡æ–™
        """
        if not contracts:
            contracts = CONTRACTS
        
        # ç¢ºä¿çµæŸæ—¥æœŸä¸è¶…éä»Šå¤©
        today = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        if end_date > today:
            logger.warning(f"çµæŸæ—¥æœŸ {end_date.strftime('%Y/%m/%d')} è¶…éä»Šå¤©ï¼Œå°‡è¨­å®šç‚ºä»Šå¤© {today.strftime('%Y/%m/%d')}")
            end_date = today
        
        # å¦‚æœé–‹å§‹æ—¥æœŸè¶…éä»Šå¤©ï¼Œå‰‡ç„¡æ³•çˆ¬å–
        if start_date > today:
            logger.warning(f"é–‹å§‹æ—¥æœŸ {start_date.strftime('%Y/%m/%d')} è¶…éä»Šå¤©ï¼Œç„¡æ³•çˆ¬å–è³‡æ–™")
            return pd.DataFrame()
        
        # è¨ˆç®—ç¸½ä»»å‹™æ•¸é‡ï¼ˆç”¨æ–¼é€²åº¦æ¢ï¼‰
        date_range = [start_date + datetime.timedelta(days=x) for x in range((end_date - start_date).days + 1)]
        # éæ¿¾åªåŒ…å«ä»Šå¤©ä¹‹å‰çš„æ—¥æœŸ
        date_range = [d for d in date_range if d <= today]
        business_days = [d for d in date_range if self._is_business_day(d)]
        
        total_tasks = len(business_days) * len(contracts)
        if identities:
            total_tasks = len(business_days) * len(contracts) * len(identities)
        
        logger.info(f"é–‹å§‹çˆ¬å–å¾ {start_date.strftime('%Y/%m/%d')} åˆ° {end_date.strftime('%Y/%m/%d')} çš„è³‡æ–™")
        logger.info(f"å…± {len(business_days)} å€‹äº¤æ˜“æ—¥ï¼Œ{len(contracts)} å€‹å¥‘ç´„é¡å‹")
        if identities:
            logger.info(f"åŒ…å« {len(identities)} ç¨®èº«ä»½åˆ¥è³‡æ–™")
        
        # æº–å‚™ä»»å‹™åˆ—è¡¨
        tasks = []
        for date in business_days:
            date_str = date.strftime('%Y/%m/%d')
            
            if identities:
                for contract in contracts:
                    for identity in identities:
                        tasks.append((date_str, contract, identity))
            else:
                for contract in contracts:
                    tasks.append((date_str, contract, None))
        
        # ä½¿ç”¨å¤šç·šç¨‹åŠ é€Ÿçˆ¬å–
        all_results = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # å‰µå»ºä»»å‹™
            future_to_task = {}
            for task in tasks:
                date_str, contract, identity = task
                future = executor.submit(self.fetch_data, date_str, contract, identity=identity)
                future_to_task[future] = task
            
            # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
            for future in tqdm(concurrent.futures.as_completed(future_to_task), 
                              total=len(future_to_task), 
                              desc="çˆ¬å–é€²åº¦"):
                date_str, contract, identity = future_to_task[future]
                try:
                    data = future.result()
                    if data:
                        all_results.append(data)
                        status = "æˆåŠŸ"
                    else:
                        status = "ç„¡è³‡æ–™"
                except Exception as e:
                    logger.error(f"è™•ç†ä»»å‹™ {date_str} {contract} {identity} æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
                    status = f"éŒ¯èª¤: {str(e)}"
                
                logger.debug(f"{date_str} {contract} {identity or 'ç¸½è¨ˆ'}: {status}")
        
        # å°‡çµæœè½‰æ›ç‚º DataFrame
        if all_results:
            df = pd.DataFrame(all_results)
            return df
        else:
            logger.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½•è³‡æ–™")
            return pd.DataFrame()

    def save_data(self, df, filename=None):
        """ä¿å­˜æ•¸æ“šåˆ° CSV å’Œ Excel æ–‡ä»¶"""
        if df.empty:
            logger.warning("æ²’æœ‰è³‡æ–™å¯ä¿å­˜")
            return
        
        # é€²è¡Œè³‡æ–™ä¸€è‡´æ€§æª¢æŸ¥
        self._verify_data_consistency(df)
        
        # å¦‚æœæœªæŒ‡å®šæ–‡ä»¶åï¼Œä½¿ç”¨ç•¶å‰æ™‚é–“ç”Ÿæˆ
        if not filename:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"taifex_data_{timestamp}"
        
        # ä¿å­˜ç‚º CSV
        csv_path = os.path.join(self.output_dir, f"{filename}.csv")
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        
        # ä¿å­˜ç‚º Excel
        excel_path = os.path.join(self.output_dir, f"{filename}.xlsx")
        df.to_excel(excel_path, index=False)
        
        logger.info(f"è³‡æ–™å·²ä¿å­˜è‡³ {csv_path} å’Œ {excel_path}")
        return csv_path, excel_path
        
    def _verify_data_consistency(self, df):
        """é©—è­‰è³‡æ–™ä¸€è‡´æ€§ï¼Œæª¢æŸ¥æ˜¯å¦æœ‰å¥‘ç´„é¡å‹æ··æ·†çš„æƒ…æ³"""
        if 'å¥‘ç´„åç¨±' not in df.columns or 'èº«ä»½åˆ¥' not in df.columns:
            return
            
        # æª¢æŸ¥æ¯å€‹èº«ä»½åˆ¥å’Œå¥‘ç´„çš„çµ„åˆ
        for identity in df['èº«ä»½åˆ¥'].dropna().unique():
            if not identity:
                continue
                
            for contract in CONTRACTS:
                contract_data = df[(df['èº«ä»½åˆ¥'] == identity) & (df['å¥‘ç´„åç¨±'] == contract)]
                
                if len(contract_data) == 0:
                    continue
                    
                # æª¢æŸ¥æ•¸æ“šæ˜¯å¦åˆç†
                has_suspicious_data = False
                
                # å¦‚æœåŒä¸€å¥‘ç´„æœ‰å¤šç­†è³‡æ–™ï¼Œå¯èƒ½æ˜¯éŒ¯èª¤è­˜åˆ¥
                if len(contract_data) > len(contract_data['æ—¥æœŸ'].unique()):
                    logger.warning(f"åµæ¸¬åˆ° {contract} {identity} å¯èƒ½æœ‰é‡è¤‡è³‡æ–™")
                    has_suspicious_data = True
                
                # æª¢æŸ¥å£æ•¸å·®è·æ˜¯å¦éå¤§ï¼ˆå¯èƒ½æ˜¯ä¸åŒå¥‘ç´„çš„è³‡æ–™è¢«éŒ¯èª¤è­˜åˆ¥ï¼‰
                if len(contract_data) > 3:
                    avg_vol = contract_data['å¤šæ–¹äº¤æ˜“å£æ•¸'].mean()
                    max_vol = contract_data['å¤šæ–¹äº¤æ˜“å£æ•¸'].max()
                    
                    if max_vol > 5 * avg_vol and max_vol > 1000:
                        logger.warning(f"åµæ¸¬åˆ° {contract} {identity} å¯èƒ½æœ‰ç•°å¸¸å€¼ï¼Œæœ€å¤§å€¼ {max_vol} é å¤§æ–¼å¹³å‡å€¼ {avg_vol}")
                        has_suspicious_data = True
                
                if has_suspicious_data:
                    logger.info(f"è«‹æª¢æŸ¥ {contract} {identity} çš„è³‡æ–™æ˜¯å¦æ­£ç¢ºï¼Œå¯èƒ½å­˜åœ¨å¥‘ç´„è­˜åˆ¥éŒ¯èª¤")

    def _find_column_indices(self, table):
        """å°‹æ‰¾è¡¨é ­ä¸­å„æ¬„ä½çš„ç´¢å¼•ä½ç½®"""
        # æ¬„ä½æ¨™é¡Œå°æ‡‰è¡¨
        column_keys = {
            'å¤šæ–¹äº¤æ˜“': {'vol': 'å¤šæ–¹äº¤æ˜“å£æ•¸', 'val': 'å¤šæ–¹å¥‘ç´„é‡‘é¡'},
            'è²·æ–¹äº¤æ˜“': {'vol': 'å¤šæ–¹äº¤æ˜“å£æ•¸', 'val': 'å¤šæ–¹å¥‘ç´„é‡‘é¡'},
            'è²·é€²äº¤æ˜“': {'vol': 'å¤šæ–¹äº¤æ˜“å£æ•¸', 'val': 'å¤šæ–¹å¥‘ç´„é‡‘é¡'},
            'ç©ºæ–¹äº¤æ˜“': {'vol': 'ç©ºæ–¹äº¤æ˜“å£æ•¸', 'val': 'ç©ºæ–¹å¥‘ç´„é‡‘é¡'},
            'è³£æ–¹äº¤æ˜“': {'vol': 'ç©ºæ–¹äº¤æ˜“å£æ•¸', 'val': 'ç©ºæ–¹å¥‘ç´„é‡‘é¡'},
            'è³£å‡ºäº¤æ˜“': {'vol': 'ç©ºæ–¹äº¤æ˜“å£æ•¸', 'val': 'ç©ºæ–¹å¥‘ç´„é‡‘é¡'},
            'å¤šç©ºæ·¨é¡äº¤æ˜“': {'vol': 'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸', 'val': 'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡'},
            'è²·è³£æ·¨é¡äº¤æ˜“': {'vol': 'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸', 'val': 'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡'},
            'äº¤æ˜“æ·¨é¡': {'vol': 'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸', 'val': 'å¤šç©ºæ·¨é¡å¥‘ç´„é‡‘é¡'},
            'å¤šæ–¹æœªå¹³å€‰': {'vol': 'å¤šæ–¹æœªå¹³å€‰å£æ•¸', 'val': 'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'è²·æ–¹æœªå¹³å€‰': {'vol': 'å¤šæ–¹æœªå¹³å€‰å£æ•¸', 'val': 'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'è²·é€²æœªå¹³å€‰': {'vol': 'å¤šæ–¹æœªå¹³å€‰å£æ•¸', 'val': 'å¤šæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'ç©ºæ–¹æœªå¹³å€‰': {'vol': 'ç©ºæ–¹æœªå¹³å€‰å£æ•¸', 'val': 'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'è³£æ–¹æœªå¹³å€‰': {'vol': 'ç©ºæ–¹æœªå¹³å€‰å£æ•¸', 'val': 'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'è³£å‡ºæœªå¹³å€‰': {'vol': 'ç©ºæ–¹æœªå¹³å€‰å£æ•¸', 'val': 'ç©ºæ–¹æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'å¤šç©ºæ·¨é¡æœªå¹³å€‰': {'vol': 'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸', 'val': 'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'è²·è³£æ·¨é¡æœªå¹³å€‰': {'vol': 'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸', 'val': 'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
            'æœªå¹³å€‰æ·¨é¡': {'vol': 'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸', 'val': 'å¤šç©ºæ·¨é¡æœªå¹³å€‰å¥‘ç´„é‡‘é¡'},
        }
        
        # å£æ•¸èˆ‡é‡‘é¡çš„é—œéµè©
        vol_keywords = ['å£æ•¸', 'æ•¸é‡', 'å£', 'äº¤æ˜“é‡', 'æˆäº¤é‡']
        val_keywords = ['é‡‘é¡', 'åƒ¹å€¼', 'å¥‘ç´„é‡‘é¡', 'å¥‘ç´„åƒ¹å€¼']
        
        # å°‹æ‰¾è¡¨é ­è¡Œ
        header_row = None
        for row in table.find_all('tr'):
            cells = row.find_all(['th', 'td'])
            if not cells:
                continue
                
            row_text = ' '.join([cell.get_text(strip=True) for cell in cells])
            # æª¢æŸ¥æ˜¯å¦åŒ…å«è¡¨é ­é—œéµå­—
            if any(key in row_text for key in column_keys.keys()):
                header_row = row
                break
        
        if not header_row:
            return {}
            
        # è§£æè¡¨é ­
        result = {}
        header_cells = header_row.find_all(['th', 'td'])
        
        for i, cell in enumerate(header_cells):
            cell_text = cell.get_text(strip=True)
            
            # æª¢æŸ¥æ¯å€‹æ¬„ä½æ¨™é¡Œ
            for key, mapping in column_keys.items():
                if key in cell_text:
                    # æª¢æŸ¥æ˜¯å£æ•¸é‚„æ˜¯é‡‘é¡
                    if any(vk in cell_text for vk in vol_keywords):
                        result[mapping['vol']] = i
                    elif any(vk in cell_text for vk in val_keywords):
                        result[mapping['val']] = i
        
        return result

    def save_complete_report(self, report_data, date_range_str, contracts):
        """å„²å­˜å®Œæ•´å ±å‘Š"""
        # å¯¦ç¾å„²å­˜å®Œæ•´å ±å‘Šçš„é‚è¼¯
        pass


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œåƒæ•¸"""
    parser = argparse.ArgumentParser(description='å°ç£æœŸè²¨äº¤æ˜“æ‰€è³‡æ–™çˆ¬å–å·¥å…·')
    
    # æ–°å¢ä¾¿æ·çš„æ—¥æœŸç¯„åœåƒæ•¸
    parser.add_argument('--date-range', type=str, 
                        help='æ—¥æœŸç¯„åœ: "today" è¡¨ç¤ºä»Šå¤©, "YYYY-MM-DD" è¡¨ç¤ºå–®æ—¥, "YYYY-MM-DD,YYYY-MM-DD" è¡¨ç¤ºç¯„åœ')
    
    # åŸæœ‰çš„æ—¥æœŸç¯„åœåƒæ•¸ï¼ˆä¿æŒå‘å¾Œç›¸å®¹ï¼‰
    parser.add_argument('--start_date', type=str, help='é–‹å§‹æ—¥æœŸ (æ ¼å¼: YYYY/MM/DD)')
    parser.add_argument('--end_date', type=str, help='çµæŸæ—¥æœŸ (æ ¼å¼: YYYY/MM/DD)')
    parser.add_argument('--year', type=int, help='è¦çˆ¬å–çš„å¹´ä»½ï¼Œä¾‹å¦‚ 2023')
    parser.add_argument('--month', type=int, help='è¦çˆ¬å–çš„æœˆä»½ï¼Œä¾‹å¦‚ 1-12')
    
    # å¥‘ç´„åƒæ•¸ - æ”¯æ´é€—è™Ÿåˆ†éš”çš„å­—ä¸²
    parser.add_argument('--contracts', type=str, default='ALL',
                        help='è¦çˆ¬å–çš„å¥‘ç´„ä»£ç¢¼ï¼Œä¾‹å¦‚ TX,TE,MTXï¼Œæˆ–ä½¿ç”¨ ALL çˆ¬å–æ‰€æœ‰å¥‘ç´„')
    
    # èº«ä»½åˆ¥åƒæ•¸
    parser.add_argument('--identities', type=str, nargs='+',
                        choices=IDENTITIES + ['ALL', 'NONE'], default=['ALL'],
                        help='è¦çˆ¬å–çš„èº«ä»½åˆ¥ï¼Œä¾‹å¦‚ è‡ªç‡Ÿå•† æŠ•ä¿¡ å¤–è³‡ï¼Œæˆ–ä½¿ç”¨ ALL çˆ¬å–æ‰€æœ‰èº«ä»½åˆ¥ï¼ŒNONE è¡¨ç¤ºä¸çˆ¬å–èº«ä»½åˆ¥è³‡æ–™')
    
    # è¼¸å‡ºåƒæ•¸
    parser.add_argument('--output_dir', type=str, default='output',
                        help='è¼¸å‡ºç›®éŒ„è·¯å¾‘')
    parser.add_argument('--filename', type=str, help='è¼¸å‡ºæª”å (ä¸å«å‰¯æª”å)')
    
    # çˆ¬èŸ²é…ç½®åƒæ•¸
    parser.add_argument('--max_workers', type=int, default=10,
                        help='æœ€å¤§å·¥ä½œç·šç¨‹æ•¸')
    parser.add_argument('--delay', type=float, default=0.2,
                        help='è«‹æ±‚é–“éš”æ™‚é–“ (ç§’)')
    parser.add_argument('--max_retries', type=int, default=3,
                        help='æœ€å¤§é‡è©¦æ¬¡æ•¸')
    
    args = parser.parse_args()
    
    # è™•ç†æ–°çš„æ—¥æœŸç¯„åœåƒæ•¸
    if args.date_range:
        if args.date_range.lower() == 'today':
            # ä»Šå¤©
            today = datetime.datetime.now()
            start_date = today
            end_date = today
        elif ',' in args.date_range:
            # æ—¥æœŸç¯„åœ: YYYY-MM-DD,YYYY-MM-DD
            dates = args.date_range.split(',')
            start_date = datetime.datetime.strptime(dates[0].strip(), "%Y-%m-%d")
            end_date = datetime.datetime.strptime(dates[1].strip(), "%Y-%m-%d")
        else:
            # å–®æ—¥: YYYY-MM-DD
            date = datetime.datetime.strptime(args.date_range, "%Y-%m-%d")
            start_date = date
            end_date = date
    elif args.year:
        # å¦‚æœæŒ‡å®šäº†å¹´ä»½
        year = args.year
        if args.month:
            # å¦‚æœåŒæ™‚æŒ‡å®šäº†æœˆä»½ï¼Œå‰‡çˆ¬å–è©²æœˆ
            month = args.month
            start_date = datetime.datetime(year, month, 1)
            if month == 12:
                end_date = datetime.datetime(year + 1, 1, 1) - datetime.timedelta(days=1)
            else:
                end_date = datetime.datetime(year, month + 1, 1) - datetime.timedelta(days=1)
        else:
            # åªæŒ‡å®šå¹´ä»½ï¼Œçˆ¬å–æ•´å¹´
            start_date = datetime.datetime(year, 1, 1)
            end_date = datetime.datetime(year, 12, 31)
    else:
        # ä½¿ç”¨æ˜ç¢ºçš„é–‹å§‹å’ŒçµæŸæ—¥æœŸ
        if not args.start_date:
            # é»˜èªç‚ºç•¶å¹´åˆè‡³ä»Š
            today = datetime.datetime.now()
            start_date = datetime.datetime(today.year, 1, 1)
            end_date = today
        else:
            # è§£æç”¨æˆ¶æä¾›çš„æ—¥æœŸ
            start_date = datetime.datetime.strptime(args.start_date, "%Y/%m/%d")
            if args.end_date:
                end_date = datetime.datetime.strptime(args.end_date, "%Y/%m/%d")
            else:
                end_date = datetime.datetime.now()
    
    # ç¢ºä¿çµæŸæ—¥æœŸä¸è¶…éä»Šå¤©
    today = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
    if end_date > today:
        end_date = today
        logger.info(f"çµæŸæ—¥æœŸè¨­å®šç‚ºä»Šå¤©: {today.strftime('%Y/%m/%d')}")
    
    args.start_date = start_date
    args.end_date = end_date
    
    # è™•ç†å¥‘ç´„é‚è¼¯ - æ”¯æ´é€—è™Ÿåˆ†éš”çš„å­—ä¸²
    if isinstance(args.contracts, str):
        if args.contracts.upper() == 'ALL':
            args.contracts = CONTRACTS
        else:
            args.contracts = [c.strip().upper() for c in args.contracts.split(',') if c.strip()]
    elif 'ALL' in args.contracts:
        args.contracts = CONTRACTS
    
    # è™•ç†èº«ä»½åˆ¥é‚è¼¯
    if 'ALL' in args.identities:
        args.identities = IDENTITIES
    elif 'NONE' in args.identities:
        args.identities = None
    
    return args


def main():
    """ä¸»ç¨‹åº"""
    args = parse_arguments()
    
    # é¡¯ç¤ºçˆ¬å–é…ç½®
    logger.info("å°ç£æœŸè²¨äº¤æ˜“æ‰€è³‡æ–™çˆ¬å–å·¥å…· (Python ç‰ˆ)")
    logger.info(f"çˆ¬å–æ—¥æœŸç¯„åœ: {args.start_date.strftime('%Y/%m/%d')} - {args.end_date.strftime('%Y/%m/%d')}")
    logger.info(f"å¥‘ç´„: {', '.join(args.contracts)}")
    logger.info(f"èº«ä»½åˆ¥: {', '.join(args.identities) if args.identities else 'ä¸çˆ¬å–èº«ä»½åˆ¥è³‡æ–™'}")
    
    # åˆå§‹åŒ–è³‡æ–™åº«ç®¡ç†å™¨
    if DB_AVAILABLE:
        db_manager = TaifexDatabaseManager()
        report_generator = DailyReportGenerator(db_manager)
        logger.info("è³‡æ–™åº«ç³»çµ±å·²å•Ÿç”¨")
    else:
        db_manager = None
        report_generator = None
    
    # åˆå§‹åŒ–Google Sheetsç®¡ç†å™¨
    if SHEETS_AVAILABLE:
        sheets_manager = GoogleSheetsManager()
        if sheets_manager.client:
            logger.info("Google Sheetsç³»çµ±å·²å•Ÿç”¨")
        else:
            logger.warning("Google Sheetsèªè­‰æœªå®Œæˆï¼Œè·³éé›²ç«¯ä¸Šå‚³")
            sheets_manager = None
    else:
        sheets_manager = None
    
    # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
    crawler = TaifexCrawler(
        output_dir=args.output_dir,
        max_workers=args.max_workers,
        delay=args.delay,
        max_retries=args.max_retries
    )
    
    # çˆ¬å–è³‡æ–™
    df = crawler.crawl_date_range(
        args.start_date,
        args.end_date,
        contracts=args.contracts,
        identities=args.identities
    )
    
    # ä¿å­˜è³‡æ–™
    if not df.empty:
        # ç”Ÿæˆé»˜èªæª”å
        if not args.filename:
            date_range = f"{args.start_date.strftime('%Y%m%d')}-{args.end_date.strftime('%Y%m%d')}"
            contracts_str = '_'.join(args.contracts)
            identity_str = '_'.join(args.identities) if args.identities else 'no_identity'
            args.filename = f"taifex_{date_range}_{contracts_str}_{identity_str}"
        
        # 1. ä¿å­˜åˆ°å‚³çµ±æª”æ¡ˆæ ¼å¼
        csv_path, excel_path = crawler.save_data(df, args.filename)
        logger.info(f"å·²æˆåŠŸçˆ¬å– {len(df)} ç­†è³‡æ–™")
        logger.info(f"CSV æª”æ¡ˆ: {csv_path}")
        logger.info(f"Excel æª”æ¡ˆ: {excel_path}")
        
        # 2. ä¿å­˜åˆ°è³‡æ–™åº«ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if db_manager and not df.empty:
            try:
                # è½‰æ›è³‡æ–™æ ¼å¼ä»¥ç¬¦åˆè³‡æ–™åº«çµæ§‹
                db_df = prepare_data_for_db(df)
                db_manager.insert_data(db_df)
                logger.info("è³‡æ–™å·²æˆåŠŸå­˜å…¥è³‡æ–™åº«")
                
                # ç”Ÿæˆ30å¤©æ—¥å ±ï¼ˆå¦‚æœè³‡æ–™è¶³å¤ ï¼‰
                recent_data = db_manager.get_recent_data(30)
                if not recent_data.empty and len(recent_data) > 50:  # ç¢ºä¿æœ‰è¶³å¤ è³‡æ–™
                    report = report_generator.generate_30day_report()
                    if report:
                        logger.info("30å¤©åˆ†æå ±å‘Šå·²ç”Ÿæˆ")
                
                # åŒ¯å‡ºæœ€æ–°30å¤©è³‡æ–™åˆ°å›ºå®šæª”æ¡ˆ
                latest_30d_path = Path(args.output_dir) / "å°æœŸæ‰€æœ€æ–°30å¤©è³‡æ–™.xlsx"
                db_manager.export_to_excel(latest_30d_path, days=30)
                logger.info(f"æœ€æ–°30å¤©è³‡æ–™å·²åŒ¯å‡º: {latest_30d_path}")
                
                # 3. ä¸Šå‚³åˆ°Google Sheetsï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if sheets_manager:
                    try:
                        # é€£æ¥æˆ–å»ºç«‹è©¦ç®—è¡¨
                        spreadsheet_config_file = Path("config/spreadsheet_config.json")
                        
                        if spreadsheet_config_file.exists():
                            # è¼‰å…¥ç¾æœ‰è©¦ç®—è¡¨é…ç½®
                            with open(spreadsheet_config_file, 'r', encoding='utf-8') as f:
                                config = json.load(f)
                                spreadsheet_id = config.get('spreadsheet_id')
                                
                                if spreadsheet_id:
                                    sheets_manager.connect_spreadsheet(spreadsheet_id)
                                    logger.info("å·²é€£æ¥åˆ°ç¾æœ‰çš„Googleè©¦ç®—è¡¨")
                        
                        if not sheets_manager.spreadsheet:
                            # å»ºç«‹æ–°è©¦ç®—è¡¨
                            spreadsheet = sheets_manager.create_spreadsheet("å°æœŸæ‰€è³‡æ–™åˆ†æ")
                            if spreadsheet:
                                # å„²å­˜è©¦ç®—è¡¨é…ç½®
                                config = {
                                    'spreadsheet_id': spreadsheet.id,
                                    'spreadsheet_url': sheets_manager.get_spreadsheet_url(),
                                    'created_at': datetime.now().isoformat()
                                }
                                
                                spreadsheet_config_file.parent.mkdir(exist_ok=True)
                                with open(spreadsheet_config_file, 'w', encoding='utf-8') as f:
                                    json.dump(config, f, indent=2, ensure_ascii=False)
                                
                                # è¨­å®šç‚ºå…¬é–‹å¯æª¢è¦–
                                sheets_manager.share_spreadsheet()
                                
                                logger.info(f"ğŸ‰ Googleè©¦ç®—è¡¨å·²å»ºç«‹: {sheets_manager.get_spreadsheet_url()}")
                                logger.info("ğŸ“± ç¾åœ¨å¯ä»¥åœ¨ä»»ä½•è£ç½®ä¸Šå­˜å–å°æœŸæ‰€è³‡æ–™äº†ï¼")
                        
                        if sheets_manager.spreadsheet:
                            # ä¸Šå‚³è³‡æ–™åˆ°Google Sheets
                            recent_data = db_manager.get_recent_data(30)
                            summary_data = db_manager.get_daily_summary(30)
                            
                            # ä¸Šå‚³ä¸»è¦è³‡æ–™
                            if not recent_data.empty:
                                sheets_manager.upload_data(recent_data)
                                logger.info("âœ… è³‡æ–™å·²ä¸Šå‚³åˆ°Google Sheets")
                            
                            # ä¸Šå‚³æ‘˜è¦è³‡æ–™
                            if not summary_data.empty:
                                sheets_manager.upload_summary(summary_data)
                                sheets_manager.update_trend_analysis(summary_data)
                                logger.info("âœ… æ‘˜è¦å’Œè¶¨å‹¢åˆ†æå·²æ›´æ–°")
                            
                            # æ›´æ–°ç³»çµ±è³‡è¨Š
                            sheets_manager.update_system_info()
                            
                            logger.info(f"ğŸŒ Googleè©¦ç®—è¡¨ç¶²å€: {sheets_manager.get_spreadsheet_url()}")
                            logger.info("ğŸ’¡ æç¤º: å°‡æ­¤ç¶²å€åŠ å…¥æ›¸ç±¤ï¼Œéš¨æ™‚æŸ¥çœ‹æœ€æ–°è³‡æ–™")
                    
                    except Exception as e:
                        logger.error(f"Google Sheetsä¸Šå‚³å¤±æ•—: {e}")
                        logger.info("æœ¬åœ°è³‡æ–™å·²æ­£å¸¸ä¿å­˜ï¼Œå¯ç¨å¾Œæ‰‹å‹•ä¸Šå‚³")
                
                # 4. ç”Ÿæˆåœ–è¡¨ä¸¦ç™¼é€åˆ°Telegramï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if CHART_AVAILABLE and TELEGRAM_AVAILABLE:
                    try:
                        logger.info("ğŸ¨ é–‹å§‹ç”Ÿæˆåœ–è¡¨...")
                        
                        # åˆå§‹åŒ–åœ–è¡¨ç”Ÿæˆå™¨
                        chart_generator = ChartGenerator(output_dir="charts")
                        
                        # å„ªå…ˆå¾Google Sheetsç²å–30å¤©æ­·å²è³‡æ–™
                        chart_data = None
                        if sheets_manager and sheets_manager.spreadsheet:
                            logger.info("ğŸ“Š å¾Google Sheetsè¼‰å…¥æ­·å²è³‡æ–™...")
                            chart_data = chart_generator.load_data_from_google_sheets(30)
                        
                        # å¦‚æœGoogle Sheetsæ²’æœ‰è³‡æ–™ï¼Œå‰‡å¾è³‡æ–™åº«ç²å–
                        if chart_data is None or chart_data.empty:
                            if db_manager:
                                logger.info("ğŸ“Š å¾è³‡æ–™åº«è¼‰å…¥æ­·å²è³‡æ–™...")
                                chart_data = db_manager.get_recent_data(30)
                            else:
                                # æœ€å¾Œå˜—è©¦å¾ç•¶å‰çˆ¬å–çš„è³‡æ–™
                                chart_data = df
                        
                        if not chart_data.empty:
                            logger.info(f"ğŸ“Š ä½¿ç”¨ {len(chart_data)} ç­†è³‡æ–™ç”Ÿæˆåœ–è¡¨")
                            
                            # ç”Ÿæˆæ‰€æœ‰åœ–è¡¨
                            chart_paths = chart_generator.generate_all_charts(chart_data)
                            
                            if chart_paths:
                                logger.info(f"ğŸ“Š å·²ç”Ÿæˆ {len(chart_paths)} å€‹åœ–è¡¨")
                                
                                # ç”Ÿæˆæ‘˜è¦æ–‡å­—
                                summary_text = chart_generator.generate_summary_text(chart_data)
                                
                                # åˆå§‹åŒ–Telegramé€šçŸ¥å™¨
                                telegram_bot_token = "7088578241:AAErbP-EuoRGClRZ3FFfPMjl8k3CFpqgn8E"
                                telegram_chat_id = "1038401606"
                                notifier = TelegramNotifier(telegram_bot_token, telegram_chat_id)
                                
                                # æ¸¬è©¦é€£ç·š
                                if notifier.test_connection():
                                    # ç™¼é€åœ–è¡¨å ±å‘Š
                                    success = notifier.send_chart_report(chart_paths, summary_text)
                                    
                                    if success:
                                        logger.info("ğŸ“± åœ–è¡¨å·²æˆåŠŸç™¼é€åˆ°Telegram")
                                    else:
                                        logger.warning("âš ï¸ Telegramç™¼é€éƒ¨åˆ†å¤±æ•—")
                                else:
                                    logger.error("âŒ Telegramé€£ç·šå¤±æ•—ï¼Œç„¡æ³•ç™¼é€åœ–è¡¨")
                            else:
                                logger.warning("âš ï¸ æ²’æœ‰ç”Ÿæˆä»»ä½•åœ–è¡¨")
                        else:
                            logger.info("ğŸ“Š æ²’æœ‰æ‰¾åˆ°è¶³å¤ çš„æ­·å²è³‡æ–™ç”Ÿæˆåœ–è¡¨")
                    
                    except Exception as e:
                        logger.error(f"åœ–è¡¨ç”Ÿæˆæˆ–Telegramç™¼é€å¤±æ•—: {e}")
                        logger.info("è³‡æ–™å·²æ­£å¸¸ä¿å­˜ï¼Œåœ–è¡¨åŠŸèƒ½å°‡è·³é")
                
                elif not CHART_AVAILABLE:
                    logger.info("ğŸ“Š åœ–è¡¨ç”Ÿæˆæ¨¡çµ„æœªå•Ÿç”¨ï¼Œè«‹å®‰è£ matplotlib")
                elif not TELEGRAM_AVAILABLE:
                    logger.info("ğŸ“± Telegramé€šçŸ¥æ¨¡çµ„æœªå•Ÿç”¨")
                elif not db_manager:
                    logger.info("ğŸ—„ï¸ è³‡æ–™åº«æœªå•Ÿç”¨ï¼Œç„¡æ³•ç”Ÿæˆ30å¤©åœ–è¡¨")
                
            except Exception as e:
                logger.error(f"è³‡æ–™åº«æ“ä½œå¤±æ•—: {e}")
        
        # å„²å­˜å®Œæ•´å ±å‘Š
        # ç§»é™¤æœªå®šç¾©çš„report_data
        
        logger.info("ç¨‹å¼åŸ·è¡Œå®Œæˆ")
        return 0  # æˆåŠŸé€€å‡º
        
    else:
        # æ²’æœ‰çˆ¬å–åˆ°è³‡æ–™
        logger.warning("âš ï¸ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•æœ‰æ•ˆè³‡æ–™")
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå‡æ—¥æˆ–éäº¤æ˜“æ—¥
        today = datetime.datetime.now()
        if today.weekday() >= 5:  # é€±å…­æ—¥
            logger.info("ä»Šæ—¥ç‚ºé€±æœ«ï¼Œå¯èƒ½æ²’æœ‰äº¤æ˜“è³‡æ–™")
            return 0  # é€±æœ«æ²’è³‡æ–™æ˜¯æ­£å¸¸çš„
        else:
            logger.error("âŒ äº¤æ˜“æ—¥ä½†æ²’æœ‰è³‡æ–™ï¼Œå¯èƒ½ç¶²ç«™æœ‰å•é¡Œæˆ–è³‡æ–™å°šæœªå…¬å¸ƒ")
            return 1  # å›å‚³éŒ¯èª¤é€€å‡ºç¢¼


def prepare_data_for_db(df):
    """å°‡çˆ¬èŸ²è³‡æ–™è½‰æ›ç‚ºè³‡æ–™åº«æ ¼å¼"""
    if df.empty:
        return pd.DataFrame()
    
    # è³‡æ–™åº«éœ€è¦çš„æ¬„ä½
    required_columns = [
        'date', 'contract_code', 'identity_type', 'position_type',
        'long_position', 'short_position', 'net_position'
    ]
    
    db_records = []
    
    for _, row in df.iterrows():
        base_record = {
            'date': row.get('äº¤æ˜“æ—¥æœŸ', ''),
            'contract_code': row.get('å¥‘ç´„', ''),
        }
        
        # è™•ç†èº«ä»½åˆ¥è³‡æ–™
        if 'èº«ä»½åˆ¥' in df.columns:
            base_record['identity_type'] = row.get('èº«ä»½åˆ¥', '')
        else:
            base_record['identity_type'] = 'ç¸½è¨ˆ'
        
        # è™•ç†å¤šæ–¹éƒ¨ä½
        if any(col for col in df.columns if 'å¤šæ–¹' in col and 'å£æ•¸' in col):
            long_cols = [col for col in df.columns if 'å¤šæ–¹' in col and 'å£æ•¸' in col]
            long_position = sum(row.get(col, 0) for col in long_cols)
            
            record_long = base_record.copy()
            record_long.update({
                'position_type': 'å¤šæ–¹',
                'long_position': long_position,
                'short_position': 0,
                'net_position': long_position
            })
            db_records.append(record_long)
        
        # è™•ç†ç©ºæ–¹éƒ¨ä½
        if any(col for col in df.columns if 'ç©ºæ–¹' in col and 'å£æ•¸' in col):
            short_cols = [col for col in df.columns if 'ç©ºæ–¹' in col and 'å£æ•¸' in col]
            short_position = sum(row.get(col, 0) for col in short_cols)
            
            record_short = base_record.copy()
            record_short.update({
                'position_type': 'ç©ºæ–¹',
                'long_position': 0,
                'short_position': short_position,
                'net_position': -short_position
            })
            db_records.append(record_short)
        
        # è™•ç†æ·¨éƒ¨ä½
        if any(col for col in df.columns if 'æ·¨éƒ¨ä½' in col):
            net_cols = [col for col in df.columns if 'æ·¨éƒ¨ä½' in col]
            net_position = sum(row.get(col, 0) for col in net_cols)
            
            record_net = base_record.copy()
            record_net.update({
                'position_type': 'æ·¨éƒ¨ä½',
                'long_position': 0,
                'short_position': 0,
                'net_position': net_position
            })
            db_records.append(record_net)
    
    if not db_records:
        # å¦‚æœæ²’æœ‰è­˜åˆ¥åˆ°æ¨™æº–æ ¼å¼ï¼Œå‰µå»ºåŸºæœ¬è¨˜éŒ„
        for _, row in df.iterrows():
            record = {
                'date': row.get('äº¤æ˜“æ—¥æœŸ', ''),
                'contract_code': row.get('å¥‘ç´„', ''),
                'identity_type': row.get('èº«ä»½åˆ¥', 'ç¸½è¨ˆ'),
                'position_type': 'æœªåˆ†é¡',
                'long_position': 0,
                'short_position': 0,
                'net_position': 0
            }
            db_records.append(record)
    
    return pd.DataFrame(db_records)


if __name__ == "__main__":
    import sys
    exit_code = main()
    sys.exit(exit_code) 