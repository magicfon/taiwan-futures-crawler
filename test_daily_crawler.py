#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¸¬è©¦æ¯æ—¥çˆ¬å–å’Œé‡è©¦æ©Ÿåˆ¶
æ¨¡æ“¬GitHub Actionsçš„è¡Œç‚º
"""

import subprocess
import time
import os
import logging
from datetime import datetime

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("æ¸¬è©¦çˆ¬èŸ²")

def check_output_files():
    """æª¢æŸ¥æ˜¯å¦æœ‰ç”¢ç”Ÿè¼¸å‡ºæª”æ¡ˆ"""
    output_dir = "output"
    if not os.path.exists(output_dir):
        return False
    
    files = os.listdir(output_dir)
    # éæ¿¾å‡ºä»Šå¤©ç”¢ç”Ÿçš„æª”æ¡ˆ
    today_str = datetime.now().strftime('%Y%m%d')
    today_files = [f for f in files if today_str in f and (f.endswith('.csv') or f.endswith('.xlsx'))]
    
    return len(today_files) > 0

def run_crawler():
    """åŸ·è¡Œçˆ¬èŸ²ä¸¦å›å‚³æ˜¯å¦æˆåŠŸ"""
    try:
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œçˆ¬èŸ²...")
        
        # åŸ·è¡Œä»Šæ—¥è³‡æ–™çˆ¬å–
        result = subprocess.run([
            'python', 'taifex_crawler.py', 
            '--date-range', 'today',
            '--contracts', 'TX,TE,MTX'
        ], capture_output=True, text=True, timeout=300)
        
        logger.info(f"çˆ¬èŸ²é€€å‡ºç¢¼: {result.returncode}")
        
        if result.stdout:
            logger.info("æ¨™æº–è¼¸å‡º:")
            for line in result.stdout.split('\n')[-10:]:  # åªé¡¯ç¤ºæœ€å¾Œ10è¡Œ
                if line.strip():
                    logger.info(f"  {line}")
        
        if result.stderr:
            logger.warning("éŒ¯èª¤è¼¸å‡º:")
            for line in result.stderr.split('\n')[-5:]:  # åªé¡¯ç¤ºæœ€å¾Œ5è¡Œ
                if line.strip():
                    logger.warning(f"  {line}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ç”¢ç”Ÿæª”æ¡ˆ
        has_files = check_output_files()
        
        # çˆ¬èŸ²æˆåŠŸæ¢ä»¶ï¼šé€€å‡ºç¢¼ç‚º0ä¸”æœ‰ç”¢ç”Ÿæª”æ¡ˆ
        success = (result.returncode == 0) and has_files
        
        if success:
            logger.info("âœ… çˆ¬èŸ²åŸ·è¡ŒæˆåŠŸï¼Œæœ‰ç”¢ç”Ÿè³‡æ–™æª”æ¡ˆ")
        else:
            if result.returncode != 0:
                logger.warning(f"âš ï¸ çˆ¬èŸ²é€€å‡ºç¢¼ç•°å¸¸: {result.returncode}")
            if not has_files:
                logger.warning("âš ï¸ æ²’æœ‰ç”¢ç”Ÿè³‡æ–™æª”æ¡ˆ")
        
        return success
        
    except subprocess.TimeoutExpired:
        logger.error("âŒ çˆ¬èŸ²åŸ·è¡Œè¶…æ™‚")
        return False
    except Exception as e:
        logger.error(f"âŒ çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {e}")
        return False

def main():
    """ä¸»æ¸¬è©¦ç¨‹å¼"""
    logger.info("=== æ¸¬è©¦æ¯æ—¥çˆ¬å–å’Œé‡è©¦æ©Ÿåˆ¶ ===")
    
    # ç¬¬ä¸€æ¬¡å˜—è©¦
    logger.info("\nğŸ“‹ ç¬¬ä¸€æ¬¡å˜—è©¦:")
    first_success = run_crawler()
    
    if first_success:
        logger.info("ğŸ‰ ç¬¬ä¸€æ¬¡å˜—è©¦æˆåŠŸï¼Œç„¡éœ€é‡è©¦")
        return 0
    
    # å¦‚æœç¬¬ä¸€æ¬¡å¤±æ•—ï¼Œç­‰å¾…å¾Œé‡è©¦
    logger.info("\nâ° ç¬¬ä¸€æ¬¡å¤±æ•—ï¼Œç­‰å¾…10ç§’å¾Œé‡è©¦...")
    time.sleep(10)  # å¯¦éš›ç’°å¢ƒæ˜¯600ç§’(10åˆ†é˜)ï¼Œæ¸¬è©¦ç’°å¢ƒç¸®çŸ­ç‚º10ç§’
    
    logger.info("\nğŸ”„ é‡è©¦å˜—è©¦:")
    retry_success = run_crawler()
    
    if retry_success:
        logger.info("ğŸ‰ é‡è©¦æˆåŠŸï¼")
        return 0
    else:
        logger.error("âŒ å…©æ¬¡å˜—è©¦éƒ½å¤±æ•—")
        return 1

def show_current_time_info():
    """é¡¯ç¤ºç•¶å‰æ™‚é–“è³‡è¨Š"""
    now = datetime.now()
    logger.info(f"ç•¶å‰æ™‚é–“: {now.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"æ˜ŸæœŸ: {['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥'][now.weekday()]}")
    
    if now.weekday() >= 5:
        logger.info("ä»Šå¤©æ˜¯é€±æœ«ï¼Œå¯èƒ½æ²’æœ‰äº¤æ˜“è³‡æ–™")
    else:
        logger.info("ä»Šå¤©æ˜¯å·¥ä½œæ—¥ï¼Œæ‡‰è©²æœ‰äº¤æ˜“è³‡æ–™")

if __name__ == "__main__":
    show_current_time_info()
    exit_code = main()
    print(f"\næœ€çµ‚é€€å‡ºç¢¼: {exit_code}")
    exit(exit_code) 