#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ­·å²è³‡æ–™çˆ¬å–è…³æœ¬
å¾æŒ‡å®šæ—¥æœŸç¯„åœçˆ¬å–å°æœŸæ‰€è³‡æ–™ï¼Œä¸¦ä¸Šå‚³åˆ°Google Sheetsæ­·å²è³‡æ–™åº«
"""

from taifex_crawler import TaifexCrawler
from google_sheets_manager import GoogleSheetsManager
import pandas as pd
from datetime import datetime, timedelta
import logging
import argparse

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def crawl_historical_data(start_date, end_date=None, contracts=None, identities=None):
    """
    çˆ¬å–æ­·å²è³‡æ–™ä¸¦ä¸Šå‚³åˆ°Google Sheets
    
    Args:
        start_date: é–‹å§‹æ—¥æœŸ (æ ¼å¼: 'YYYY-MM-DD')
        end_date: çµæŸæ—¥æœŸ (æ ¼å¼: 'YYYY-MM-DD')ï¼Œé è¨­ç‚ºä»Šå¤©
        contracts: å¥‘ç´„åˆ—è¡¨ï¼Œé è¨­ç‚º ['TX', 'TE', 'MTX', 'ZMX', 'NQF']
        identities: èº«ä»½åˆ¥åˆ—è¡¨ï¼Œé è¨­ç‚º ['è‡ªç‡Ÿå•†', 'æŠ•ä¿¡', 'å¤–è³‡']
    """
    if not end_date:
        end_date = datetime.now().strftime('%Y-%m-%d')
    
    if not contracts:
        contracts = ['TX', 'TE', 'MTX', 'ZMX', 'NQF']  # åŒ…å«æ‰€æœ‰ä¸»è¦æœŸè²¨å¥‘ç´„
    
    if not identities:
        identities = ['è‡ªç‡Ÿå•†', 'æŠ•ä¿¡', 'å¤–è³‡']
    
    print(f"ğŸš€ é–‹å§‹çˆ¬å–æ­·å²è³‡æ–™...")
    print(f"ğŸ“… æ—¥æœŸç¯„åœ: {start_date} è‡³ {end_date}")
    print(f"ğŸ“Š å¥‘ç´„: {', '.join(contracts)}")
    print(f"ğŸ‘¥ èº«ä»½åˆ¥: {', '.join(identities)}")
    
    # åˆå§‹åŒ–çˆ¬èŸ²
    crawler = TaifexCrawler(output_dir="output_history", delay=1.0)
    
    # è½‰æ›æ—¥æœŸæ ¼å¼ç‚ºdatetimeç‰©ä»¶
    try:
        start_date_obj = datetime.strptime(start_date, '%Y-%m-%d')
        end_date_obj = datetime.strptime(end_date, '%Y-%m-%d')
    except ValueError as e:
        print(f"âŒ æ—¥æœŸæ ¼å¼éŒ¯èª¤: {e}")
        return pd.DataFrame()
    
    # çˆ¬å–è³‡æ–™
    all_data = []
    
    for contract in contracts:
        print(f"\nğŸ“ˆ æ­£åœ¨çˆ¬å– {contract} å¥‘ç´„è³‡æ–™...")
        
        try:
            # çˆ¬å–æ—¥æœŸç¯„åœçš„è³‡æ–™
            contract_data = crawler.crawl_date_range(
                start_date=start_date_obj,
                end_date=end_date_obj,
                contracts=[contract],
                identities=identities
            )
            
            if contract_data is not None and not contract_data.empty:
                print(f"âœ… {contract} çˆ¬å–æˆåŠŸ: {len(contract_data)} ç­†è³‡æ–™")
                all_data.append(contract_data)
            else:
                print(f"âš ï¸ {contract} æ²’æœ‰è³‡æ–™")
                
        except Exception as e:
            print(f"âŒ {contract} çˆ¬å–å¤±æ•—: {e}")
            logger.error(f"çˆ¬å– {contract} å¤±æ•—: {e}")
    
    # åˆä½µæ‰€æœ‰è³‡æ–™
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"\nğŸ“Š ç¸½å…±çˆ¬å–åˆ° {len(combined_df)} ç­†è³‡æ–™")
        
        # å„²å­˜æœ¬åœ°CSV
        filename = f"output_history/taifex_history_{start_date}_{end_date}.csv"
        combined_df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ æœ¬åœ°å‚™ä»½å·²å„²å­˜: {filename}")
        
        # ä¸Šå‚³åˆ°Google Sheets
        print(f"\nğŸŒ æ­£åœ¨ä¸Šå‚³åˆ°Google Sheets...")
        
        try:
            # é€£æ¥Google Sheets
            manager = GoogleSheetsManager()
            
            if not manager.client:
                print("âŒ Google Sheetsé€£æ¥å¤±æ•—")
                return combined_df
            
            # é€£æ¥åˆ°è©¦ç®—è¡¨
            spreadsheet_url = "https://docs.google.com/spreadsheets/d/1w1uslujf-DF7BufO6s5TPYAjvWgUS3B_jCczDxrhmA4"
            spreadsheet = manager.connect_spreadsheet(spreadsheet_url)
            
            if not spreadsheet:
                print("âŒ é€£æ¥è©¦ç®—è¡¨å¤±æ•—")
                return combined_df
            
            # æª¢æŸ¥æ­·å²è³‡æ–™å·¥ä½œè¡¨æ˜¯å¦å­˜åœ¨
            worksheet_names = [ws.title for ws in spreadsheet.worksheets()]
            
            if "æ­·å²è³‡æ–™" not in worksheet_names:
                print("âš ï¸ æ­·å²è³‡æ–™å·¥ä½œè¡¨ä¸å­˜åœ¨ï¼Œæ­£åœ¨å»ºç«‹...")
                try:
                    worksheet = spreadsheet.add_worksheet(title="æ­·å²è³‡æ–™", rows=10000, cols=20)
                    headers = manager.get_history_headers()
                    worksheet.update('A1', [headers])
                    worksheet.format('A1:Z1', {
                        'backgroundColor': {'red': 0.2, 'green': 0.6, 'blue': 0.9},
                        'textFormat': {'bold': True, 'foregroundColor': {'red': 1, 'green': 1, 'blue': 1}}
                    })
                    print("âœ… æ­·å²è³‡æ–™å·¥ä½œè¡¨å»ºç«‹æˆåŠŸ!")
                except Exception as e:
                    print(f"âŒ å»ºç«‹æ­·å²è³‡æ–™å·¥ä½œè¡¨å¤±æ•—: {e}")
                    return combined_df
            
            # ä¸Šå‚³è³‡æ–™
            success = manager.upload_data(combined_df, "æ­·å²è³‡æ–™")
            
            if success:
                print("âœ… è³‡æ–™å·²æˆåŠŸä¸Šå‚³åˆ°Google Sheets!")
                print(f"ğŸŒ æŸ¥çœ‹çµæœ: {manager.get_spreadsheet_url()}")
            else:
                print("âŒ ä¸Šå‚³åˆ°Google Sheetså¤±æ•—")
            
        except Exception as e:
            print(f"âŒ Google Sheetsè™•ç†å¤±æ•—: {e}")
            logger.error(f"Google Sheetsè™•ç†å¤±æ•—: {e}")
        
        return combined_df
    
    else:
        print("âŒ æ²’æœ‰çˆ¬å–åˆ°ä»»ä½•è³‡æ–™")
        return pd.DataFrame()

def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    parser = argparse.ArgumentParser(description='å°æœŸæ‰€æ­·å²è³‡æ–™çˆ¬å–å·¥å…·')
    parser.add_argument('--start-date', '-s', type=str, default='2025-05-01',
                        help='é–‹å§‹æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œé è¨­ç‚º 2025-05-01')
    parser.add_argument('--end-date', '-e', type=str, default=None,
                        help='çµæŸæ—¥æœŸ (æ ¼å¼: YYYY-MM-DD)ï¼Œé è¨­ç‚ºä»Šå¤©')
    parser.add_argument('--contracts', '-c', nargs='+', default=['TX', 'TE', 'MTX', 'ZMX', 'NQF'],
                        help='å¥‘ç´„ä»£ç¢¼æ¸…å–®ï¼Œé è¨­ç‚º TX,TE,MTX,ZMX,NQF')
    parser.add_argument('--identities', '-i', nargs='+', 
                        default=['è‡ªç‡Ÿå•†', 'æŠ•ä¿¡', 'å¤–è³‡'],
                        help='èº«ä»½åˆ¥æ¸…å–®ï¼Œé è¨­ç‚ºå…¨éƒ¨')
    
    args = parser.parse_args()
    
    # åŸ·è¡Œçˆ¬å–
    result_df = crawl_historical_data(
        start_date=args.start_date,
        end_date=args.end_date,
        contracts=args.contracts,
        identities=args.identities
    )
    
    if not result_df.empty:
        print(f"\nğŸ‰ æ­·å²è³‡æ–™çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“Š ç¸½ç­†æ•¸: {len(result_df)}")
        print(f"ğŸ“… æ—¥æœŸç¯„åœ: {result_df['æ—¥æœŸ'].min()} è‡³ {result_df['æ—¥æœŸ'].max()}")
        
        # é¡¯ç¤ºæ‘˜è¦çµ±è¨ˆ
        print(f"\nğŸ“‹ è³‡æ–™æ‘˜è¦:")
        for identity in result_df['èº«ä»½åˆ¥'].unique():
            identity_data = result_df[result_df['èº«ä»½åˆ¥'] == identity]
            print(f"  {identity}: {len(identity_data)} ç­†è³‡æ–™")
    else:
        print("ğŸ˜ æ²’æœ‰çˆ¬å–åˆ°è³‡æ–™")

if __name__ == "__main__":
    main() 