#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æª¢æŸ¥è¿‘30å¤©äº¤æ˜“æ—¥è³‡æ–™éºæ¼ä¸¦è‡ªå‹•è£œé½Š
åœ¨GitHub Actionsæ¯æ—¥çˆ¬å–ä¹‹å‰åŸ·è¡Œ
"""

import sys
import os
import logging
from datetime import datetime, timedelta
import pandas as pd
import sqlite3
from database_manager import TaifexDatabaseManager
from google_sheets_manager import GoogleSheetsManager
import subprocess

# è¨­å®šæ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('missing_data_check.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MissingDataChecker:
    """éºæ¼è³‡æ–™æª¢æŸ¥å’Œè£œé½Šå·¥å…·"""
    
    def __init__(self):
        self.db = TaifexDatabaseManager()
        try:
            self.gm = GoogleSheetsManager()
            self.has_google_sheets = True
        except Exception as e:
            logger.warning(f"ç„¡æ³•åˆå§‹åŒ–Google Sheets: {e}")
            self.has_google_sheets = False
        
        # é è¨­å¥‘ç´„ä»£ç¢¼
        self.default_contracts = ['TX', 'TE', 'MTX']
        
    def is_trading_day(self, date):
        """æª¢æŸ¥æ˜¯å¦ç‚ºäº¤æ˜“æ—¥ï¼ˆéé€±æœ«ï¼‰"""
        return date.weekday() < 5  # 0-4 æ˜¯é€±ä¸€åˆ°é€±äº”
    
    def get_trading_days_in_range(self, start_date, end_date):
        """å–å¾—æŒ‡å®šç¯„åœå…§çš„æ‰€æœ‰äº¤æ˜“æ—¥"""
        trading_days = []
        current_date = start_date
        
        while current_date <= end_date:
            if self.is_trading_day(current_date):
                trading_days.append(current_date)
            current_date += timedelta(days=1)
        
        return trading_days
    
    def check_database_missing_dates(self, days=30):
        """æª¢æŸ¥è³‡æ–™åº«ä¸­éºæ¼çš„äº¤æ˜“æ—¥"""
        logger.info(f"ğŸ” æª¢æŸ¥è¿‘{days}å¤©çš„äº¤æ˜“æ—¥è³‡æ–™...")
        
        # è¨ˆç®—æª¢æŸ¥ç¯„åœ
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days)
        
        # å–å¾—æ‰€æœ‰æ‡‰è©²æœ‰è³‡æ–™çš„äº¤æ˜“æ—¥
        expected_trading_days = self.get_trading_days_in_range(start_date, end_date)
        logger.info(f"ğŸ“… æœŸé–“å…§æ‡‰æœ‰äº¤æ˜“æ—¥: {len(expected_trading_days)}å¤©")
        
        # æŸ¥è©¢è³‡æ–™åº«ä¸­ç¾æœ‰çš„æ—¥æœŸ
        conn = sqlite3.connect(self.db.db_path)
        query = """
        SELECT DISTINCT date 
        FROM futures_data 
        WHERE date >= ? AND date <= ?
        """
        
        existing_dates_df = pd.read_sql_query(
            query, 
            conn, 
            params=[
                start_date.strftime('%Y/%m/%d'),
                end_date.strftime('%Y/%m/%d')
            ]
        )
        conn.close()
        
        # è½‰æ›ç¾æœ‰æ—¥æœŸç‚ºdatetimeç‰©ä»¶
        existing_dates = set()
        for date_str in existing_dates_df['date']:
            try:
                # è™•ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
                if '/' in date_str:
                    if len(date_str.split('/')[1]) == 1:  # 2025/6/5 æ ¼å¼
                        date_obj = datetime.strptime(date_str, '%Y/%m/%d')
                    else:  # 2025/06/05 æ ¼å¼
                        date_obj = datetime.strptime(date_str, '%Y/%m/%d')
                    existing_dates.add(date_obj.date())
            except ValueError as e:
                logger.warning(f"ç„¡æ³•è§£ææ—¥æœŸæ ¼å¼: {date_str} - {e}")
        
        # æ‰¾å‡ºéºæ¼çš„äº¤æ˜“æ—¥
        missing_dates = []
        for trading_day in expected_trading_days:
            if trading_day.date() not in existing_dates:
                missing_dates.append(trading_day)
        
        logger.info(f"ğŸ“Š è³‡æ–™åº«ç¾æœ‰äº¤æ˜“æ—¥: {len(existing_dates)}å¤©")
        logger.info(f"âŒ éºæ¼çš„äº¤æ˜“æ—¥: {len(missing_dates)}å¤©")
        
        if missing_dates:
            logger.info("éºæ¼çš„æ—¥æœŸ:")
            for missing_date in sorted(missing_dates):
                weekday_names = ['é€±ä¸€', 'é€±äºŒ', 'é€±ä¸‰', 'é€±å››', 'é€±äº”', 'é€±å…­', 'é€±æ—¥']
                weekday = weekday_names[missing_date.weekday()]
                logger.info(f"   ğŸ“… {missing_date.strftime('%Y/%m/%d')} ({weekday})")
        
        return missing_dates
    
    def crawl_missing_data(self, missing_dates):
        """çˆ¬å–éºæ¼çš„è³‡æ–™"""
        if not missing_dates:
            logger.info("âœ… æ²’æœ‰éºæ¼çš„è³‡æ–™éœ€è¦è£œé½Š")
            return True
        
        logger.info(f"ğŸš€ é–‹å§‹è£œé½Š {len(missing_dates)} å¤©çš„éºæ¼è³‡æ–™...")
        
        success_count = 0
        failed_dates = []
        
        for date in sorted(missing_dates):
            try:
                date_str = date.strftime('%Y-%m-%d')
                logger.info(f"ğŸ“ˆ æ­£åœ¨çˆ¬å– {date_str} çš„è³‡æ–™...")
                
                # åŸ·è¡Œçˆ¬èŸ²
                cmd = [
                    'python', 'taifex_crawler.py',
                    '--date-range', f'{date_str},{date_str}',
                    '--contracts', ','.join(self.default_contracts)
                ]
                
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    encoding='utf-8',
                    timeout=300  # 5åˆ†é˜è¶…æ™‚
                )
                
                if result.returncode == 0:
                    logger.info(f"âœ… {date_str} è³‡æ–™çˆ¬å–æˆåŠŸ")
                    success_count += 1
                else:
                    logger.error(f"âŒ {date_str} è³‡æ–™çˆ¬å–å¤±æ•—: {result.stderr}")
                    failed_dates.append(date)
                
            except subprocess.TimeoutExpired:
                logger.error(f"âŒ {date_str} çˆ¬å–è¶…æ™‚")
                failed_dates.append(date)
            except Exception as e:
                logger.error(f"âŒ {date_str} çˆ¬å–éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
                failed_dates.append(date)
        
        logger.info(f"ğŸ“Š è£œé½Šçµæœ: æˆåŠŸ {success_count} å¤©, å¤±æ•— {len(failed_dates)} å¤©")
        
        if failed_dates:
            logger.warning("å¤±æ•—çš„æ—¥æœŸ:")
            for failed_date in failed_dates:
                logger.warning(f"   âŒ {failed_date.strftime('%Y/%m/%d')}")
        
        return len(failed_dates) == 0
    
    def sync_to_google_sheets(self):
        """åŒæ­¥è³‡æ–™åˆ°Google Sheets"""
        if not self.has_google_sheets:
            logger.warning("âš ï¸ è·³éGoogle SheetsåŒæ­¥ï¼ˆèªè­‰æœªè¨­å®šï¼‰")
            return True
        
        try:
            logger.info("ğŸ“Š é–‹å§‹åŒæ­¥è³‡æ–™åˆ°Google Sheets...")
            
            # å–å¾—æœ€è¿‘30å¤©çš„è³‡æ–™
            recent_data = self.db.get_recent_data(days=30)
            
            if recent_data.empty:
                logger.warning("âš ï¸ æ²’æœ‰è³‡æ–™å¯ä»¥åŒæ­¥åˆ°Google Sheets")
                return False
            
            # è½‰æ›ç‚ºGoogle Sheetsæ ¼å¼
            sheets_data = self.prepare_sheets_data(recent_data)
            
            # ä¸Šå‚³åˆ°Google Sheets
            success = self.gm.upload_data(sheets_data)
            
            if success:
                logger.info("âœ… Google SheetsåŒæ­¥æˆåŠŸ")
                return True
            else:
                logger.error("âŒ Google SheetsåŒæ­¥å¤±æ•—")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Google SheetsåŒæ­¥éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def prepare_sheets_data(self, df):
        """æº–å‚™Google Sheetsæ ¼å¼çš„è³‡æ–™"""
        # è½‰æ›ç‚ºGoogle Sheetséœ€è¦çš„æ ¼å¼
        sheets_data = []
        
        for _, row in df.iterrows():
            sheets_row = {
                'æ—¥æœŸ': row['date'],
                'å¥‘ç´„åç¨±': row['contract_code'],
                'èº«ä»½åˆ¥': row['identity_type'],
                'å¤šæ–¹äº¤æ˜“å£æ•¸': row['long_position'] if row['position_type'] == 'å¤šæ–¹' else 0,
                'ç©ºæ–¹äº¤æ˜“å£æ•¸': row['short_position'] if row['position_type'] == 'ç©ºæ–¹' else 0,
                'å¤šæ–¹æœªå¹³å€‰å£æ•¸': row['long_position'] if row['position_type'] == 'å¤šæ–¹' else 0,
                'ç©ºæ–¹æœªå¹³å€‰å£æ•¸': row['short_position'] if row['position_type'] == 'ç©ºæ–¹' else 0,
                'å¤šç©ºæ·¨é¡äº¤æ˜“å£æ•¸': row['net_position'],
                'å¤šç©ºæ·¨é¡æœªå¹³å€‰å£æ•¸': row['net_position'],
                'æ›´æ–°æ™‚é–“': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            sheets_data.append(sheets_row)
        
        return pd.DataFrame(sheets_data)
    
    def run_complete_check(self, days=30):
        """åŸ·è¡Œå®Œæ•´çš„æª¢æŸ¥å’Œè£œé½Šæµç¨‹"""
        logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œå®Œæ•´çš„è³‡æ–™æª¢æŸ¥å’Œè£œé½Šæµç¨‹...")
        
        try:
            # 1. æª¢æŸ¥éºæ¼çš„äº¤æ˜“æ—¥
            missing_dates = self.check_database_missing_dates(days)
            
            # 2. è£œé½Šéºæ¼çš„è³‡æ–™
            if missing_dates:
                crawl_success = self.crawl_missing_data(missing_dates)
                if not crawl_success:
                    logger.warning("âš ï¸ éƒ¨åˆ†è³‡æ–™è£œé½Šå¤±æ•—ï¼Œä½†ç¹¼çºŒåŸ·è¡Œ")
            
            # 3. åŒæ­¥åˆ°Google Sheets
            sync_success = self.sync_to_google_sheets()
            
            # 4. ç”Ÿæˆå ±å‘Š
            self.generate_report(missing_dates, days)
            
            logger.info("âœ… å®Œæ•´æª¢æŸ¥å’Œè£œé½Šæµç¨‹åŸ·è¡Œå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æª¢æŸ¥å’Œè£œé½Šæµç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
            return False
    
    def generate_report(self, missing_dates, days):
        """ç”Ÿæˆæª¢æŸ¥å ±å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆæª¢æŸ¥å ±å‘Š...")
        
        report = {
            'check_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'check_period_days': days,
            'missing_dates_count': len(missing_dates),
            'missing_dates': [d.strftime('%Y-%m-%d') for d in missing_dates],
            'database_status': 'healthy' if len(missing_dates) == 0 else 'has_gaps',
            'google_sheets_status': 'enabled' if self.has_google_sheets else 'disabled'
        }
        
        # å„²å­˜å ±å‘Š
        import json
        os.makedirs('reports', exist_ok=True)
        
        with open('reports/missing_data_check_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“‹ å ±å‘Šå·²å„²å­˜åˆ° reports/missing_data_check_report.json")
        
        # åœ¨æ—¥èªŒä¸­è¼¸å‡ºæ‘˜è¦
        logger.info("=" * 50)
        logger.info("ğŸ“Š æª¢æŸ¥çµæœæ‘˜è¦:")
        logger.info(f"   æª¢æŸ¥æœŸé–“: è¿‘{days}å¤©")
        logger.info(f"   éºæ¼äº¤æ˜“æ—¥: {len(missing_dates)}å¤©")
        logger.info(f"   è³‡æ–™åº«ç‹€æ…‹: {report['database_status']}")
        logger.info(f"   Google Sheets: {report['google_sheets_status']}")
        logger.info("=" * 50)

def main():
    """ä¸»ç¨‹å¼"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æª¢æŸ¥ä¸¦è£œé½Šè¿‘æœŸéºæ¼çš„äº¤æ˜“æ—¥è³‡æ–™')
    parser.add_argument('--days', type=int, default=30, help='æª¢æŸ¥å¤©æ•¸ (é è¨­: 30)')
    parser.add_argument('--skip-sync', action='store_true', help='è·³éGoogle SheetsåŒæ­¥')
    
    args = parser.parse_args()
    
    logger.info("ğŸ” é–‹å§‹åŸ·è¡Œéºæ¼è³‡æ–™æª¢æŸ¥å’Œè£œé½Š...")
    
    try:
        checker = MissingDataChecker()
        
        if args.skip_sync:
            # åªæª¢æŸ¥å’Œè£œé½Šï¼Œä¸åŒæ­¥
            missing_dates = checker.check_database_missing_dates(args.days)
            if missing_dates:
                checker.crawl_missing_data(missing_dates)
            checker.generate_report(missing_dates, args.days)
        else:
            # åŸ·è¡Œå®Œæ•´æµç¨‹
            success = checker.run_complete_check(args.days)
            
            if success:
                logger.info("âœ… æ‰€æœ‰æª¢æŸ¥å’Œè£œé½Šä»»å‹™å®Œæˆ")
                sys.exit(0)
            else:
                logger.error("âŒ æª¢æŸ¥å’Œè£œé½Šéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤")
                sys.exit(1)
                
    except Exception as e:
        logger.error(f"âŒ ç¨‹å¼åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main() 